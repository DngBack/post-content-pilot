{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response='The capital of France is Paris.'\n"
     ]
    }
   ],
   "source": [
    "from src.settings import openai_service\n",
    "from src.module import OpenAIInput\n",
    "from src.module import OpenAIService\n",
    "from src.settings.settings import load_settings\n",
    "\n",
    "settings = load_settings()\n",
    "\n",
    "openai_service = OpenAIService(settings=settings.openai)\n",
    "\n",
    "input_data = OpenAIInput(user_prompt=\"What is the capital of France?\", system_prompt=\"Context: The capital of France is Paris.\")\n",
    "\n",
    "response = openai_service.process(inputs=input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.module import LlamaParserInput\n",
    "from src.module import LlamaParserService\n",
    "from src.settings.settings import load_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.settings.settings import load_settings\n",
    "\n",
    "settings = load_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = LlamaParserService(settings=settings.llamaparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 052e0aa6-a0ef-44a7-b913-ece4fac433f6\n"
     ]
    }
   ],
   "source": [
    "documents = parser.process(\n",
    "    inputs=LlamaParserInput(\n",
    "        file_path=\"./data/2502.19634.pdf\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# arXiv:2502.19634v1  [cs.CV]  26 Feb 2025\n",
      "\n",
      "# MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning\n",
      "\n",
      "Jiazhen Pan1,2∗, Che Liu3∗, Junde Wu2, Fenglin Liu2, Jiayun Zhu2, Hongwei Bran Li4, Chen Chen5,6, Cheng Ouyang2,6†, Daniel Rueckert1,6†\n",
      "\n",
      "1Chair for AI in Healthcare and Medicine, Technical University of Munich (TUM) and TUM University Hospital, Germany\n",
      "\n",
      "2Department of Engineering Science, University of Oxford, UK\n",
      "\n",
      "3Data Science Institute, Imperial College London, UK\n",
      "\n",
      "4Massachusetts General Hospital, Harvard Medical School, USA\n",
      "\n",
      "5School of Computer Science, University of Sheffield, UK\n",
      "\n",
      "6Department of Computing, Imperial College London, UK\n",
      "\n",
      "# Abstract\n",
      "\n",
      "Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show promise for radiological tasks, most existing VLMs merely produce final answers without revealing the underlying reasoning. To address this gap, we introduce MedVLM-R1, a medical VLM that explicitly generates natural language reasoning to enhance transparency and trustworthiness. Instead of relying on supervised fine-tuning (SFT), which often suffers from overfitting to training distributions and fails to foster genuine reasoning, MedVLM-R1 employs a reinforcement learning framework that incentivizes the model to discover human-interpretable reasoning paths without using any reasoning references. Despite limited training data (600 visual question answering samples) and model parameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming larger models trained on over a million samples. It also demonstrates robust domain generalization under out-of-distribution tasks. By unifying medical image analysis with explicit reasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable AI in clinical practice.\n",
      "\n",
      "# Keywords\n",
      "\n",
      "Medical reasoning · Reinforcement learning · VLMs\n",
      "\n",
      "# 1 Introduction\n",
      "\n",
      "Radiological images are fundamental to modern healthcare, with over 8 billion scans performed annually [2]. As diagnostic demand grows, the demand for\n",
      "\n",
      "∗Equal contribution\n",
      "\n",
      "†Equal advice\n"
     ]
    }
   ],
   "source": [
    "print(documents.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "post-content-pilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
