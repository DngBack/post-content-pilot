{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARNING)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# # Uncomment if you want to temporarily disable logger\n",
    "# logger = logging.getLogger()\n",
    "# logger.disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, get_response_synthesizer\n",
    "from llama_index.core import DocumentSummaryIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI__OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 2ebc470a-565b-4565-bd7e-6789d5d827a8\n",
      ".[Document(id_='bd46e53a-e560-4e06-b90f-f98b3b038197', embedding=None, metadata={'file_path': 'data\\\\test_data.pdf', 'file_name': 'test_data.pdf', 'file_type': 'application/pdf', 'file_size': 1172290, 'creation_date': '2025-03-01', 'last_modified_date': '2025-03-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# arXiv:2502.19634v1  [cs.CV]  26 Feb 2025\\n\\n# MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning\\n\\nJiazhen Pan1,2∗, Che Liu3∗, Junde Wu2, Fenglin Liu2, Jiayun Zhu2, Hongwei Bran Li4, Chen Chen5,6, Cheng Ouyang2,6†, Daniel Rueckert1,6†\\n\\n1Chair for AI in Healthcare and Medicine, Technical University of Munich (TUM) and TUM University Hospital, Germany\\n\\n2Department of Engineering Science, University of Oxford, UK\\n\\n3Data Science Institute, Imperial College London, UK\\n\\n4Massachusetts General Hospital, Harvard Medical School, USA\\n\\n5School of Computer Science, University of Sheffield, UK\\n\\n6Department of Computing, Imperial College London, UK\\n\\n# Abstract\\n\\nReasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show promise for radiological tasks, most existing VLMs merely produce final answers without revealing the underlying reasoning. To address this gap, we introduce MedVLM-R1, a medical VLM that explicitly generates natural language reasoning to enhance transparency and trustworthiness. Instead of relying on supervised fine-tuning (SFT), which often suffers from overfitting to training distributions and fails to foster genuine reasoning, MedVLM-R1 employs a reinforcement learning framework that incentivizes the model to discover human-interpretable reasoning paths without using any reasoning references. Despite limited training data (600 visual question answering samples) and model parameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming larger models trained on over a million samples. It also demonstrates robust domain generalization under out-of-distribution tasks. By unifying medical image analysis with explicit reasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable AI in clinical practice.\\n\\n# Keywords\\n\\nMedical reasoning · Reinforcement learning · VLMs\\n\\n# 1 Introduction\\n\\nRadiological images are fundamental to modern healthcare, with over 8 billion scans performed annually [2]. As diagnostic demand grows, the demand for\\n\\n∗Equal contribution\\n\\n†Equal advice', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='848e5ddf-9d9e-458a-878a-4bf48ca2329b', embedding=None, metadata={'file_path': 'data\\\\test_data.pdf', 'file_name': 'test_data.pdf', 'file_type': 'application/pdf', 'file_size': 1172290, 'creation_date': '2025-03-01', 'last_modified_date': '2025-03-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Pan, Liu et al.\\n\\nEfficient AI-driven interpretation becomes increasingly acute. Medical Vision-Language Models (VLMs), developed for radiological visual question answering (VQA) in MRI, CT and X-ray images, offer substantial promise in assisting clinicians/patients. Recent advances in general-purpose LLMs/VLMs (e.g., GPT-4o [16], Claude-3.7 Sonnet [3]) highlight sophisticated reasoning capabilities. However, the medical domain places an especially high premium on explainable decision-making: both clinicians/patients need to understand not just what conclusion was reached, but also why. Existing medical VLMs often provide only final answers or “quasi-explanations” derived from pre-training pattern matching, which do not necessarily reflect genuine, step-by-step reasoning. Consequently, ensuring interpretability and trustworthiness remains an urgent challenge in real-world clinical settings.\\n\\nWe argue that the limited reasoning capability for existing medical VLM is primarily due to the inherent drawbacks of Supervised Fine-Tuning (SFT) [1, 25, 9] which is the most common strategy for adapting large foundation models for specialized medical tasks [13, 21, 6]. Despite its simplicity, SFT faces two critical challenges: 1) An over-reliance on final-answer supervision often leads to overfitting, shortcut learning, and weaker performance on out-of-distribution (OOD) data – an issue particularly consequential in high-stake medical scenarios [11]. 2) Direct supervision with only final answers provides minimal incentive for cultivating reasoning abilities within VLMs. A possible mitigation is distilling a more capable teacher model’s chain-of-thought (CoT) reasoning for SFT [31, 20]. However, constructing high-quality CoT data is prohibitively expensive to scale in specialized domains like healthcare. As a result, current medical VLMs that rely on SFT often fall short of delivering transparent explanations and robust generalizations when confronted with unfamiliar data.\\n\\nIn contrast, Reinforcement Learning (RL) [26] offers a compelling alternative for cultivating emergent reasoning by rewarding models for discovering their own logical steps rather than memorizing final answers or copying teacher CoT rationales. Indeed, a recent work SFT Memorizes, RL Generalizes [11] confirms that RL-trained models often display superior generalization compared to their SFT counterparts. However, conventional RL pipelines typically depend on auxiliary neural reward models, requiring substantial resources to continuously update both policy and reward models [35, 24]. A promising alternative, group relative policy optimization (GRPO) [27], eliminates the need for neural reward models by employing a rule-based group-relative advantage strategy (see sec. 3 for more details). This approach has demonstrated advanced reasoning, fostering capabilities while reducing computational demands in DeepSeek-R1 [12]. Despite its potential benefits for resource- and data-constrained domains like healthcare, GRPO remains largely unexplored in medical contexts.\\n\\nIn this work, we introduce MedVLM-R1, the first medical VLM capable of generating answers with explicit reasoning by training with GRPO for radiology VQA tasks. Our contributions are as follows:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='fb124efb-41a3-4ebf-84de-55393fb00350', embedding=None, metadata={'file_path': 'data\\\\test_data.pdf', 'file_name': 'test_data.pdf', 'file_type': 'application/pdf', 'file_size': 1172290, 'creation_date': '2025-03-01', 'last_modified_date': '2025-03-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# 1. Medical VLM with Explicit Reasoning\\n\\nWe introduce MedVLM-R1, the first lightweight medical VLM capable of generating explicit reasoning alongside the final answer, rather than providing only the final answer.\\n\\n# 2. Emerging Reasoning Without Explicit Supervision\\n\\nUnlike traditional SFT methods that require data with complex reasoning steps, MedVLM-R1 is trained using GRPO with datasets containing only final answers, demonstrating emergent reasoning capabilities without explicit supervision.\\n\\n# 3. Superior Generalization and Efficiency\\n\\nMedVLM-R1 achieves robust generalization to out-of-distribution data (e.g. MRI → CT/X-ray) and outperforms larger models like Qwen2VL-72B and Huatuo-GPT-Vision-7B, despite being a compact 2B-parameter model trained on just 600 samples.\\n\\n# 2 Related Work\\n\\n# Medical VLMs and Their Limitations\\n\\nThe rise of large-scale VLMs has spurred numerous domain-specific adaptations for healthcare, with systems such as LLaVA-Med [19] and HuatuoGPT-Vision [7] achieving impressive results in radiology VQA and related diagnostic tasks. Despite these advancements, using SFT on final-answer labels remains the dominant strategy for tailoring large models to medical domains [34, 6, 33, 5]. This approach generally requires substantial amounts of high-quality image-text data (ranging from 660k [19] to 32M samples [32]) which is costly to curate and often hampered by noise/privacy concerns. Moreover, the reliance on final-answer supervision provides limited scope for exposing a model’s intermediate reasoning—an important factor in building clinicians’ trust. In addition, SFT-based models often overfit to narrow training distributions, leading to weaker generalization on OOD clinical scenarios.\\n\\n# Reinforcement Learning for Enhanced Reasoning\\n\\nTo mitigate SFT’s limitations, RL [29, 10, 35, 24, 17] has emerged as a compelling alternative for improving model interpretability and robustness. Classic RL methods, such as proximal policy optimization (PPO) [26], have been widely adopted in text-based learning (e.g., policy shaping for LLMs) and can reward not only correctness but also the quality of intermediate reasoning steps. Recent studies suggest that while SFT “memorizes,” RL can help models “generalize” [11], offering a more stable trajectory toward domain-transferable representations. Notably, GRPO [27] extends PPO by eliminating its (neural) value function estimator and focusing on a rule-based group-relative advantage for selecting actions, showing promise in resource-constrained settings like DeepSeek-R1 [12]. Such RL-driven frameworks could be particularly beneficial for medical tasks, where limited data availability, high-stakes decision-making, and the need for explicit reasoning converge. In the following section, we will detail how these insights motivate our approach.\\n\\n# 3 Methods\\n\\n# Overview\\n\\nWe leverage RL to incentivize explicit reasoning capabilities in medical VLMs, specifically employing GRPO due to its efficiency and effectiveness.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3dfb9400-41ed-4deb-ac78-683d22aa70a0', embedding=None, metadata={'file_path': 'data\\\\test_data.pdf', 'file_name': 'test_data.pdf', 'file_type': 'application/pdf', 'file_size': 1172290, 'creation_date': '2025-03-01', 'last_modified_date': '2025-03-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# 4 Pan, Liu et al.\\n\\n# Question Template\\n\\nWhat can be observed in this image?\\n\\n- A) Cartilage degeneration\\n- B) Labral pathology\\n- C) Bone fracture\\n- D) Tendonitis\\n\\n# Model Input Prompt\\n\\n{Question} Your task:\\n\\n1. Think through the question step by step, enclose your reasoning process in <think>...</think> tags.\\n2. Then provide the correct single-letter choice (A, B, C, D,...) inside <answer>...</answer> tags.\\n3. No extra information or text outside of these tags.\\n\\n# Model Output Examples\\n\\n<think> The image is a grayscale MRI image of an upper arm joint. The bicondylar humeral head of the humerus is visible. There is a well-defined ... </think>\\n\\n<answer>B, there is no clear indication of ... </answer>\\n\\nFormat Reward = 1 due to the present of all tags and no content outside the tags\\n\\nAccuracy Reward = 0.5 due to extra explanation appended after the answer\\n\\n# Fig. 1:\\n\\nThe template of our employed prompt, an example of model’s response and reward criterion.\\n\\nWhile the seminal work [27] applies GRPO to reasoning in coding and mathematics, we adapt these principles to the medical domain, specifically radiology data (MRI, CT, X-ray). Our approach incorporates medical imaging prompts and custom reward functions designed to encourage explicit reasoning and domain-specific answer formats. This work serves as an initial exploration of using pure reinforcement learning to build a multi-modal medical reasoning model.\\n\\n# Base Model and Prompt Template.\\n\\nWe adopt a state-of-the-art VLM – Qwen2-VL-2B [30] as our base model, denoted by πθ, where θ are the trainable parameters. Given a training dataset V, each sample v consists of:\\n\\n1. An image f, which is a radiology image\\n2. a text prompt q, composed of the user’s question alongside a fixed system message, as illustrated in Figure 1.\\n\\nThe VLM then produces an output {o}, which includes both a reasoning trace and a final answer in designated XML-like tags (<think>...</think> and <answer>...</answer>). Our RL objective is to optimize πθ so that answers are accurate, well-formatted, and provide transparent reasoning.\\n\\n# Group Relative Policy Optimization (GRPO).\\n\\nTo encourage robust, interpretable responses, we employ GRPO [27], an RL algorithm that extends PPO by focusing on a group-relative advantage instead of a learned value function. Concretely, at each training step:\\n\\n1. We sample G candidate outputs {oi}G from πθ, the model parameters before the current update.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='97d08b8e-ae50-4c47-ac8c-187508802aad', embedding=None, metadata={'file_path': 'data\\\\test_data.pdf', 'file_name': 'test_data.pdf', 'file_type': 'application/pdf', 'file_size': 1172290, 'creation_date': '2025-03-01', 'last_modified_date': '2025-03-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# 2\\n\\nWe compute a reward ri for each output using a reward function (see next paragraph). Based on ri we calculate a group relative advantage Ai which is normalized by the group statistics: Ai = ri−mean({r1,r2,··· ,rG}). A reward above the group average is advantaged and can further incentivize the model.\\n\\n# 3\\n\\nOur VLM model πθ is then updated by maximizing JGRP O which incorporates a clipped regularization on the relative advantage estimation for model preference alignment and training stability:\\n\\nJGRP O(θ) = Ev∼P(V)E{oi}G∼πθ(·|v)\\n\\n1/G ∑i=1G min rratio Ai,clip rratio,1 ±ϵ Ai −β DKL(πθ||πref)\\n\\nwith rratio = πθ(oi|v) . An additional Kullback–Leibler term DKL(πθ∥πref) is applied to penalize divergence from a reference model πref (the initial checkpoint), helping prevent catastrophic forgetting. ϵ, β ∈ R ≥ 0 control the regularization strengths.\\n\\n# Reward function\\n\\nSpecifically, for multiple-choice medical VQA tasks, we propose a two-part rule-based reward function, inspired by [12]:\\n\\n1. Format Reward. We incentivize outputs that provide a reasoning trace within the tags &lt;think&gt; ... &lt;/think&gt; and a succinct final answer within the tag &lt;answer&gt; ... &lt;/answer&gt;. If all four tags are present exactly once and no content is present outside these tags, we assign a format reward of 1. Any missing/duplicated tags or content outside yield 0.\\n2. Accuracy Reward. After verifying the correct format, we evaluate the correctness of the final answer. Specifically, If the letter choice A, B, C, D,... inside the &lt;answer&gt; ... &lt;/answer&gt; tag and it responds with the ground-truth choice exactly, that is an exact match with a reward of 1 point. Further, if the letter is correct but contains additional explanations or uses the choice content instead of the corresponding letter (e.g., “A: Pulmonary nodule” or \"Pulmonary nodule\"), that is a partial match with a 0.5 points reward. However, if the letter does not match, is missing, or the answer is not enclosed in the answer tag, that is an incorrect or missing answer and no reward would be granted (0 points).\\n\\nThe total reward ri ∈ [0,2] is the sum of both format and accuracy reward. By structuring the reward function in this hierarchy (format before correctness), we guide the model to first adopt the desired response structure, then refine its answer selection for accurate, interpretable medical reasoning. It is worth noting that both terms are necessary since without Format Reward, the final answer cannot be extracted while without Accuracy Reward, the model cannot converge.\\n\\n# 4\\n\\nExperiments\\n\\nDataset. We conduct our experiments using the HuatuoGPT-Vision [7] evaluation dataset, which is a processed and combined dataset from several publicly', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3662f37f-a5b8-42f2-9e3a-14bdfa6ca655', embedding=None, metadata={'file_path': 'data\\\\test_data.pdf', 'file_name': 'test_data.pdf', 'file_type': 'application/pdf', 'file_size': 1172290, 'creation_date': '2025-03-01', 'last_modified_date': '2025-03-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6 Pan, Liu et al.\\n\\navailable medical VQA benchmarks, including VQA-RAD [18], SLAKE [22], PathVQA [14], OmniMedVQA [15], and PMC-VQA [34]. In total, the dataset comprises 17,300 multiple-choice questions linked to images covering various medical imaging modalities, with 2–6 possible choices per question. For this study, we focus on radiology modalities: CT, MRI, and X-ray. Specifically, we use 600 MRI image-question pairs for training and set aside 300 MRI, 300 CT, and 300 X-ray pairs for testing. The MRI test set is used as in-domain test, whereas the CT and X-ray test set serve as OOD test.\\n\\n# Implementation details\\n\\nWe adopt Qwen2-VL-2B as our base VLM. This model is originally trained on data from curated web pages, open-source datasets, and synthetic sources. To adapt it to the medical domain, we employ the GRPO reinforcement learning framework outlined in Section 3. Our implementation builds on the public VLM reasoning repositories [23, 8, 28]. We perform fine-tuning on two NVIDIA A100 SXM4 80GB for 300 steps, using a batch size of 2, which takes approximately 4 hours. Generation candidate number G is set to 6. The other training optimization hyper-parameters are set as suggested by [8].\\n\\n# Baseline methods and evaluation metric\\n\\nWe compare MedVLM-R1 with the following baselines:\\n\\n1. Qwen2-VL family [4] including Qwen2-VL-2B (the unmodified base model), Qwen2-VL-7B and -72B which are the large/huge model variants.\\n2. HuatuoGPT-vision [7]: A medical VLM built upon Qwen2-VL-7B.\\n3. SFT: The same Qwen2-VL-2B base model fine-tuned with standard SFT, using the same training setting with 600 MRI question-answer pairs. We apply negative log-likelihood as the loss function to carry out the SFT training.\\n\\nAll baselines use a simple prompting format, e.g., {Question} Your task: provide the correct single-letter choice (A, B, C, D, ...). In contrast, MedVLM-R1 uses the RL-based prompt as described in Section 3, designed to elicit explicit reasoning. For evaluation, each model receives one point for the correct single-letter answer and zero otherwise. In the test of MedVLM-R1, only the correct choice enclosed in the &lt;answer&gt;...&lt;/answer&gt; tag is scored as correct; any deviation from this format, even if semantically correct, results in a zero score.\\n\\n# 5 Results and Discussion\\n\\n# Overall Performance\\n\\nTable 1 summarizes both in-domain (ID) and out-of-domain (OOD) performance for various VLMs. Note that ID/OOD comparisons specifically refer to models fine-tuned on MRI data. Unsurprisingly, VLMs fine-tuned with both GRPO and SFT significantly outperform zero-shot general-purpose VLMs on in-domain tasks. Our GRPO-trained model shows very strong OOD performance, achieving a 16% improvement on CT and a 35% improvement on X-ray compared to SFT counterparts, underscoring GRPO’s superior generalizability. Furthermore, despite being a compact 2B-parameter model trained on just 600 samples, MedVLM-R1 outperforms larger models like Qwen2-VL-72B and HuatuoGPT-Vision-7B, with the latter being specifically', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='fc47ed91-cfd7-42ae-967c-3e1f29d0262b', embedding=None, metadata={'file_path': 'data\\\\test_data.pdf', 'file_name': 'test_data.pdf', 'file_type': 'application/pdf', 'file_size': 1172290, 'creation_date': '2025-03-01', 'last_modified_date': '2025-03-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# MedVLM-R1\\n\\n# Problem 1: What can be observed in this image?\\n\\nA) Lungs B) Bladder C) Brain D) Heart\\n\\n<think>The image is a chest X-ray, which is a type of radiographic image used to visualize the internal structures of the body, particularly the lungs and bones. The presence of lung markings and the ribcage are characteristic features of a chest X-ray.</think>\\n\\n<answer>A</answer> Groundtruth Answer: A\\n\\n# Problem 2: What content appears in this image?\\n\\nA) Cardiac tissue, B) Breast tissue, C) Liver tissue, D) Skin tissue\\n\\n<think>The image appears to be a mammogram, which is a type of X-ray used to detect abnormalities in the breast tissue. The image shows the breast tissue with various densities and patterns, which are typical of mammograms.</think>\\n\\n<answer>B</answer> Groundtruth Answer: B\\n\\n# Problem 3: What is the finding in this image?\\n\\nA) Stomach ulcer, B) Nerve compression, C) Cord pathology, D) Liver cirrhosis\\n\\n<think>To determine the correct answer, we need to identify the finding in the image provided. The MRI scan in question is likely a view of a spine, not the stomach, or the liver. Stomach ulcers, liver cirrhosis, and spinal cord pathology can be diagnosed with MRI, but the image in question shows a spinal view, not a gastrointestinal or abdominal view. Therefore, the only reasonable options left are nerve compression and cord pathology.</think>\\n\\n<answer>C</answer> Groundtruth Answer: C\\n\\n# Problem 4: What is the specific abnormality present in this image?\\n\\nA) Gastritis, B) Arthritis, C) Appendicitis, D) Urolithiasis\\n\\n<think>The image presented is a transverse CT scan of the abdomen and pelvis. The presence of calculi (urines filled with stones or grit) in the pelvic organs is a consistent finding in urolithiasis.</think>\\n\\n<answer>D</answer> Groundtruth Answer: D\\n\\nFig. 2: Medical VQA examples of MedVLM-R1 on X-ray (1, 2), MRI (3) and CT (4). trained on large-scale medical data. This highlights the immense potential of RL-based training methods for efficient and scalable medical VLM development.\\n\\nReasoning Competence and Interpretability. Beyond strong generalization, a central strength of MedVLM-R1 is its ability to produce explicit reasoning—a capability absent in all baselines. As illustrated in Figure 2, MedVLM-R1 presents a logical thought process within the <think> tag, with the final decision enclosed in the <answer> tag. Notably, for relatively simpler questions (prob-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='fdd57869-3a73-4e59-a9cd-94cff9c9b756', embedding=None, metadata={'file_path': 'data\\\\test_data.pdf', 'file_name': 'test_data.pdf', 'file_type': 'application/pdf', 'file_size': 1172290, 'creation_date': '2025-03-01', 'last_modified_date': '2025-03-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# 8 Pan, Liu et al.\\n\\n# Table 1:\\n\\nResults of VQA-VLMs on MRI (in-domain), and CT and X-Ray (out-of-domain) modalities. \"−2B\" indicates the model has 2 billion parameters, etc.\\n\\n|Method|Num. of Seen Medical Sample|In-Domain (MRI→MRI)|Out-of-Domain (MRI→CT)|Out-of-Domain (MRI→X-ray)|Average|\\n|---|---|---|---|---|---|\\n|Random Guess|/|25.00|30.25|26.00|27.08|\\n| | |Zero-shot VLM| | | |\\n|Qwen2-VL-2B|/|61.67|50.67|53.00|55.11|\\n|Qwen2-VL-7B|/|72.33|68.67|66.63|69.21|\\n|Qwen2-VL-72B|/|68.67|60.67|72.33|67.22|\\n| | |Zero-shot Medical VLM| | | |\\n|Huatuo-GPT-vision-7B|1,294,062|71.00|63.00|73.66|69.22|\\n| | |MRI fine-tuned VLM| | | |\\n|Qwen2-VL-2B (SFT)|600|94.00|54.33|34.00|59.44|\\n|Ours-2B (GRPO)|600|95.33|70.33|69.00|78.22|\\n\\nHowever, more complex queries sometimes reveal heuristic or just partial reasoning. For example, in the third sample, the model arrives at the correct answer via the process of elimination rather than detailed medical analysis, suggesting it leverages cue-based reasoning instead of domain expertise. Likewise, in some instances (e.g., question 4), the causal chain between reasoning and conclusion remains unclear, raising the question of whether the model merely retrofits an explanation after predicting the correct answer. Despite these imperfections, MedVLM-R1 represents a notable step toward interpretability in radiological decision-making.\\n\\n# Limitations.\\n\\nAlthough MedVLM-R1 demonstrates promising results in MRI, CT, and X-ray datasets, several limitations remain:\\n\\n1. Modality Gaps: When tested on other medical modalities (e.g., pathology or OCT images), the model fails to converge. We hypothesize this arises from the base model’s insufficient exposure to such modalities during pre-training.\\n2. Closed-Set Dependence: The current approach is tailored to multiple-choice (closed-set) VQA. In open-ended question settings where no predefined options are provided, the model’s performance degrades substantially. This is also a common challenge for many VLMs.\\n3. Superficial/hallucinated Reasoning: In some reasoning cases, MedVLM-R1 provides a correct answer without offering a meaningful reasoning process (e.g., <think>To determine the correct observation from this spine MRI, let’s analyze the image.</think><answer>B</answer>). Moreover, sometimes the model concludes a correct choice while providing an inference that can lead to another answer. This phenomenon underscores that even models designed for explainability can occasionally revert to superficial/hallucinated justifications, highlighting an ongoing challenge in generating consistently transparent and logically sound rationales. Regarding all these issues, we believe the current 2B-parameter scale of our base model constitutes a potential bottleneck, and we plan to evaluate MedVLM-R1 on larger VLM backbones to address these concerns.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='14ae5a94-4f97-4255-a81d-64c4c8f6a0b1', embedding=None, metadata={'file_path': 'data\\\\test_data.pdf', 'file_name': 'test_data.pdf', 'file_type': 'application/pdf', 'file_size': 1172290, 'creation_date': '2025-03-01', 'last_modified_date': '2025-03-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# 6 Conclusion\\n\\nWe present MedVLM-R1, a medical VLM that integrates GRPO-based reinforcement learning to bridge the gap between accuracy, interpretability, and robust performance in radiology VQA. By focusing on explicit reasoning, the model fosters transparency and trustworthiness—qualities essential in high-stakes clinical environments. Our results demonstrate that RL-based approaches generalize better than purely SFT methods, particularly under OOD settings. Although VLM-based medical reasoning is still at a nascent stage and faces considerable challenges, we believe that its potential for delivering safer, more transparent AI-driven healthcare solutions will be appreciated and should be encouraged.\\n\\n# 7 Acknowledgements\\n\\nThis work is partially funded by the European Research Council (ERC) project Deep4MI (884622). Mr. Wu is supported by the Engineering and Physical Sciences Research Council (EPSRC) under grant EP/S024093/1 and GE Healthcare. Mr. Liu is supported by the Clarendon Fund. Ms. Zhu is supported by the Engineering and Physical Sciences Research Council (EPSRC) under grant EP/S024093/1 and Global Health R&D of the healthcare business of Merck KGaA, Darmstadt, Germany, Ares Trading S.A. (an affiliate of Merck KGaA, Darmstadt, Germany), Eysins, Switzerland (Crossref Funder ID: 10.13039 / 100009945). Dr. Li is supported by a Postdoc Mobility Grant from SNSF. Dr. Chen is funded by Royal Society (RGS/R2/242355). Dr. Ouyang is supported by UKRI grant EP/X040186/1.\\n\\n# References\\n\\n1. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)\\n2. Akhter, Y., Singh, R., Vatsa, M.: Ai-based radiodiagnosis using chest x-rays: A review. Frontiers in big data 6, 1120989 (2023)\\n3. Anthropic: Claude 3.7 sonnet system card (2025)\\n4. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966 (2023)\\n5. Chaves, J.M.Z., Huang, S.C., Xu, Y., Xu, H., Usuyama, N., Zhang, S., Wang, F., Xie, Y., Khademi, M., Yang, Z., et al.: Towards a clinically accessible radiology foundation model: open-access and lightweight, with automated evaluation. arXiv preprint arXiv:2403.08002 (2024)\\n6. Chen, J., Yang, D., Jiang, Y., Li, M., Wei, J., Hou, X., Zhang, L.: Efficiency in focus: Layernorm as a catalyst for fine-tuning medical visual language pre-trained models. arXiv preprint arXiv:2404.16385 (2024)\\n7. Chen, J., Ouyang, R., Gao, A., Chen, S., Chen, G.H., Wang, X., Zhang, R., Cai, Z., Ji, K., Yu, G., Wan, X., Wang, B.: Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale (2024)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7c760d4f-6d08-4198-9a3b-4a1754a5303b', embedding=None, metadata={'file_path': 'data\\\\test_data.pdf', 'file_name': 'test_data.pdf', 'file_type': 'application/pdf', 'file_size': 1172290, 'creation_date': '2025-03-01', 'last_modified_date': '2025-03-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1. Pan, Liu et al.\\n2. Chen, L., Li, L., Zhao, H., Song, Y., Vinci: R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/R1-V (2025), accessed: 2025-02-02\\n3. Chen, W., Ma, X., Wang, X., Cohen, W.W.: Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588 (2022)\\n4. Christiano, P.F., Leike, J., Brown, T., Martic, M., Legg, S., Amodei, D.: Deep reinforcement learning from human preferences. Advances in neural information processing systems 30 (2017)\\n5. Chu, T., Zhai, Y., Yang, J., Tong, S., Xie, S., Schuurmans, D., Le, Q.V., Levine, S., Ma, Y.: Sft memorizes, rl generalizes: A comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161 (2025)\\n6. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al.: Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025)\\n7. Hartsock, I., Rasool, G.: Vision-language models for medical report generation and visual question answering: A review. Frontiers in Artificial Intelligence 7 (2024)\\n8. He, X., Zhang, Y., Mou, L., Xing, E., Xie, P.: Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286 (2020)\\n9. Hu, Y., Li, T., Lu, Q., Shao, W., He, J., Qiao, Y., Luo, P.: Omnimedvqa: A new large-scale comprehensive evaluation benchmark for medical lvlm. In: Conference on Computer Vision and Pattern Recognition. pp. 22170–22183 (2024)\\n10. Hurst, A., Lerer, A., Goucher, A.P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al.: Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024)\\n11. Kumar, A., Zhuang, V., Agarwal, R., Su, Y., Co-Reyes, J.D., Singh, A., Baumli, K., Iqbal, S., Bishop, C., Roelofs, R., et al.: Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917 (2024)\\n12. Lau, J.J., Gayen, S., Ben Abacha, A., et al.: A dataset of clinically generated visual questions and answers about radiology images. Scientific data 5(1), 1–10 (2018)\\n13. Li, C., Wong, C., Zhang, S., Usuyama, N., Liu, H., Yang, J., Naumann, T., Poon, H., Gao, J.: Llava-med: Training a large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems 36, 28541–28564 (2023)\\n14. Li, L.H., Hessel, J., Yu, Y., Ren, X., Chang, K.W., Choi, Y.: Symbolic chain-of-thought distillation: Small models can also \"think\" step-by-step. arXiv preprint arXiv:2306.14050 (2023)\\n15. Lian, C., Zhou, H.Y., Yu, Y., Wang, L.: Less could be better: Parameter-efficient fine-tuning advances medical vision foundation models. arXiv preprint arXiv:2401.12215 (2024)\\n16. Liu, B., Zhan, L.M., Xu, L., Ma, L., Yang, Y., Wu, X.M.: Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering. In: international symposium on biomedical imaging (ISBI). pp. 1650–1654 (2021)\\n17. LMMs-Lab: open-r1-multimodal. https://github.com/EvolvingLMMs-Lab/open-r1-multimodal (2025), accessed: 2025-01-27\\n18. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow instructions with human feedback. Advances in neural information processing systems 35, 27730–27744 (2022)\\n19. Qwen-Team: Qwq: Reflect deeply on the boundaries of the unknown (2024)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5bfb8a8b-198c-4ce7-8629-66b846da0be5', embedding=None, metadata={'file_path': 'data\\\\test_data.pdf', 'file_name': 'test_data.pdf', 'file_type': 'application/pdf', 'file_size': 1172290, 'creation_date': '2025-03-01', 'last_modified_date': '2025-03-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# References\\n\\n1. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017)\\n2. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al.: Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024)\\n3. Shen, H., Zhang, Z., Zhang, Q., Xu, R., Zhao, T.: Vlm-r1: A stable and generalizable r1-style large vision-language model. https://github.com/om-ai-lab/VLM-R1 (2025), accessed: 2025-02-15\\n4. Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al.: Mastering the game of go without human knowledge. Nature 550(7676), 354–359 (2017)\\n5. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al.: Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution. arXiv preprint arXiv:2409.12191 (2024)\\n6. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35, 24824–24837 (2022)\\n7. Wu, C., Zhang, X., Zhang, Y., Wang, Y., Xie, W.: Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data. arXiv preprint arXiv:2308.02463 (2023)\\n8. Zhang, K., Zhou, R., Adhikarla, E., Yan, Z., Liu, Y., Yu, J., Liu, Z., Chen, X., Davison, B.D., Ren, H., et al.: A generalist vision–language foundation model for diverse biomedical tasks. Nature Medicine pp. 1–13 (2024)\\n9. Zhang, X., Wu, C., Zhao, Z., Lin, W., Zhang, Y., Wang, Y., Xie, W.: Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415 (2023)\\n10. Ziegler, D.M., Stiennon, N., Wu, J., Brown, T.B., Radford, A., Amodei, D., Christiano, P., Irving, G.: Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 (2019)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
     ]
    }
   ],
   "source": [
    "from llama_cloud_services import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# set up parser\n",
    "parser = LlamaParse(\n",
    "    api_key=os.getenv(\"LLAMAPARSER__API_KEY\"),\n",
    "    result_type=\"markdown\"  # \"markdown\" and \"text\" are available\n",
    ")\n",
    "\n",
    "# use SimpleDirectoryReader to parse our file\n",
    "file_extractor = {\".pdf\": parser}\n",
    "documents = SimpleDirectoryReader(input_files=['data/test_data.pdf'], file_extractor=file_extractor).load_data()\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM (gpt-3.5-turbo)\n",
    "chatgpt = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI__OPENAI_API_KEY\"),\n",
    "    model=os.getenv(\"OPENAI__OPENAI_MODEL\"))\n",
    "splitter = SentenceSplitter(chunk_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\post-content-pilot\\lib\\site-packages\\llama_index\\core\\llms\\utils.py:42\u001b[0m, in \u001b[0;36mresolve_llm\u001b[1;34m(llm, callback_manager)\u001b[0m\n\u001b[0;32m     41\u001b[0m     llm \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mvalidate_openai_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\post-content-pilot\\lib\\site-packages\\llama_index\\llms\\openai\\utils.py:579\u001b[0m, in \u001b[0;36mvalidate_openai_api_key\u001b[1;34m(api_key)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[1;32m--> 579\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[1;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# default mode of building the index\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m response_synthesizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_response_synthesizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtree_summarize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m doc_summary_index \u001b[38;5;241m=\u001b[39m DocumentSummaryIndex\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[0;32m      6\u001b[0m     documents,\n\u001b[0;32m      7\u001b[0m     llm\u001b[38;5;241m=\u001b[39mchatgpt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\post-content-pilot\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\factory.py:58\u001b[0m, in \u001b[0;36mget_response_synthesizer\u001b[1;34m(llm, prompt_helper, text_qa_template, refine_template, summary_template, simple_template, response_mode, callback_manager, use_async, streaming, structured_answer_filtering, output_cls, program_factory, verbose)\u001b[0m\n\u001b[0;32m     55\u001b[0m summary_template \u001b[38;5;241m=\u001b[39m summary_template \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_TREE_SUMMARIZE_PROMPT_SEL\n\u001b[0;32m     57\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m callback_manager \u001b[38;5;129;01mor\u001b[39;00m Settings\u001b[38;5;241m.\u001b[39mcallback_manager\n\u001b[1;32m---> 58\u001b[0m llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\n\u001b[0;32m     59\u001b[0m prompt_helper \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     prompt_helper\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m Settings\u001b[38;5;241m.\u001b[39m_prompt_helper\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     )\n\u001b[0;32m     65\u001b[0m )\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response_mode \u001b[38;5;241m==\u001b[39m ResponseMode\u001b[38;5;241m.\u001b[39mREFINE:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\post-content-pilot\\lib\\site-packages\\llama_index\\core\\settings.py:36\u001b[0m, in \u001b[0;36m_Settings.llm\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the LLM.\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\post-content-pilot\\lib\\site-packages\\llama_index\\core\\llms\\utils.py:49\u001b[0m, in \u001b[0;36mresolve_llm\u001b[1;34m(llm, callback_manager)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`llama-index-llms-openai` package not found, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease run `pip install llama-index-llms-openai`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m         )\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 49\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load OpenAI model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTo disable the LLM entirely, set llm=None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m         )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     60\u001b[0m     splits \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: \n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******"
     ]
    }
   ],
   "source": [
    "# default mode of building the index\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\", use_async=True\n",
    ")\n",
    "doc_summary_index = DocumentSummaryIndex.from_documents(\n",
    "    documents,\n",
    "    llm=chatgpt,\n",
    "    transformations=[splitter],\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "post-content-pilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
